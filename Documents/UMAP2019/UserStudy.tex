To complement and validate the results of the offline evaluation, we ran a user study with 24 subjects to measure the performance and preference of users with different clustering algorithms for a visual search interface.  In the following, we first describe the user study methodology followed by an evaluation of user performance.

\subsection{Methodology}
The main goal of this user study was to comparatively evaluate human performance and preference using three different search interfaces for identifying facts about natural disaster data in the previously described Twitter dataset.  The primary two hypotheses that we aimed to evaluate with human subjects were the following: ({\bf H1}) a clustering based interface leads to better search performance and is more preferred than a non-clustering interface, and ({\bf H2}) the relevance- and interface-driven clustering of {\bf BPS} leads to better search performance and is more preferred than {\bf $\mathbf{K}$-means}.

Over a sequence of three trials each using a randomized (non-overlapping) selection of three natural disasters chosen from Table~\ref{tbl:events}, each of our users was asked to use one of the following three different search approaches (with the order of the three search approaches randomized for the three trials of each user):
\begin{enumerate}[leftmargin=*]
\item A {\bf Baseline} method which displays all tweets that match the query (an example is shown in  Figure~\ref{Fig:GlobalDisplay}) with relevance shown as a gray-level shading.
\item The {\bf $\mathbf{K}$-means} algorithm discussed previously in the offline evaluation (using $X$-means to automatically identify the best $K$), which displays the largest 6  clusters for {\bf results matching a query} (an example is shown in Figure~\ref{fig:ScreenShot}).
\item The {\bf BPS} algorithm we proposed for relevance-driven clustering which displays the top 6 EF1 clusters for a query (an example with 3 clusters is shown in Figure~\ref{Fig:BPS_FilteredDisplay}).
\end{enumerate}
Though use of the {\bf Baseline} method would be visually apparent to users, users were not aware of which clustering algorithm they were using in a trial that used either {\bf $\mathbf{K}$-means} or {\bf BPS} clustering.

%trained to use the different search algorithms to find three different natural disasters.% using both a baseline interface showing all results as well as a clustering interface.

For each search approach, the interface allows users to enter a multi-term search query $q$, which returns the set $E$ of top-ranking 1,000 tweets according to their probability of relevance.
In the user experiments, we used the standard information retrieval Language Model scoring method as defined in~\cite{Zhai2001}, where $S(j) = p(q|d_j) = \prod_{i=1}^n p(q_i|d_j)$ for user query $q$ and tweet ``document'' $d_j$ with $p(q_i|d_j)$ calculated from the corpus according to a Bayesian smoothing language model using Dirichlet priors (\cite{Zhai2001}, Section~3). 

In the {\bf Baseline} search approach, these tweets are displayed on a Google Maps display used to browse the results.
The tweets that match a query were represented using circles with a grayscale color range corresponding to the probability of relevance -- light gray circles represented low probability relevance tweets, and dark gray circles represented high probability relevance tweets.
The user was able to interact with the map by panning and zooming and also by clicking on tweets to see their content.
The clustering search approaches ({\bf $\mathbf{K}$-means} and {\bf BPS}) were identical to the
{\bf Baseline} search approach, except that instead of showing all matching results, they showed six clickable clusters of results as illustrated in Figure~\ref{fig:ScreenShot} (clicking a cluster displays a summary view and clicking a tweet in the cluster displays the specific tweet content).
In all search approaches, the user could use a time slider bar to restrict the results to tweets matching the query in a specific time window. 
%Once each user had familiarized themselves with the interface, they were asked to find three other natural disasters using a different display algorithm (baseline, $k$-means, EF1 relevance-driven clustering using the BPS greedy algorithm) for each of three trials containing three natural disasters each. 

Before starting the experiment, each user was shown a training video describing the visual search interface and how to interact with the clusters.  Then, each user was tested on their ability to find each of three natural disasters from Table~\ref{tbl:events} 
%(disjoint from the disasters used in the three experimental trials) 
using both the {\bf Baseline} non-clustering interface as well as the {\bf $\mathbf{K}$-means} clustering interface in two training trials.
In each of the two training trials and the three experimental trials, the user was asked to enter information related to each natural disaster they identified, including the type of the natural disaster (e.g., earthquake, hurricane, flood, etc.) selected from a drop-down list, its location (US state) selected from a drop-down list, and the date (day) on which they think the disaster first occurred selected from a calendar chooser.
It is important to reiterate that none of the three experimental trials reused data from a previous trial.
%Once users chose correctly in the two training trials, they were allowed to proceed to the three experimental trials.
%by correctly identifying the time (day), location (US state), and type (earthquake, hurricane, etc.) of the natural disaster, 
A total of 24 users participated in the user study, for which the full experiment 
%two training and three experimental trials 
took on average 50 minutes per user.

We collected detailed interaction logs to record different behaviors and actions of the user. Moreover, each user was asked to answer NASA Task Load Index (NASA-TLX)~\cite{HART1988139} and System Usability Scale (SUS)~\cite{brooke1996sus} questionnaires after each trial. Finally, at the end of the experiment, each user was asked to fill out an exit survey, which included a preference ranking of the algorithms.

%The probability relevance score $S(j)$ of each tweet is an estimation of the conditional probability $p(q|t)$ that a tweet $t$ is relevant to the query $q$. To obtain this probability estimate, we used a language model scoring approach, which assumes that the individual terms $q_i$ of a query $q$ are ``generated'' by a probabilistic model based on the content of tweet $t$, thus directly estimating $p(q|t)$. 
%Also, for each user the sequence of algorithms to evaluate was presented differently, such that to cover all possible six combinations uniformly four times (24 users).  Here, we wish to indicate that the user had no indication of what algorithm was used each time, especially between the BPS and the $k$-means algorithms, which both present the results in a similar way using clusters (bounding boxes). 
%Finally, we highlight the fact that during we have set up the task such that it wasn't too easy nor too difficult for the user. 



% Figure \ref{fig:ScreenShot} shows the interface of the user study, with the results of the query ``tornado, blizzard, earthquake''  using $k$-means. The results of the same query with the baseline and the BPS algorithms are shown in Figure \ref{Fig:UseCase}. At first glance, we clearly observe that the clusters of the BPS algorithm are more focused and more contextualized than those of the $k$-means algorithm.  In the following, we analyze the main outcomes of this user study.



\begin{figure}[t]
\begin{centering}
{\includegraphics[width=8.5cm]{imgs/kmeans}}
\par\end{centering}
\caption{Clustering-based visual search interface. The results of the query ``tornado, blizzard, earthquake''  are shown using clusters of $K$-means, which can be hidden/shown using the top buttons. The sidebar is used to provide answers.}
\label{fig:ScreenShot}
\end{figure}


\begin{figure}[t]
\begin{centering}
\includegraphics[width=7.5cm]{imgs/legend3}
\par\end{centering}
\begin{centering}
{\includegraphics[width=8.5cm]{imgs/nd_recall}}
\par\end{centering}
\caption{The mean user performance and 95\% confidence intervals for each of the three search interfaces/algorithms are measured using cumulative recall for the type and location of the natural disasters and the absolute error for the first date of the natural disaster.  On average, users achieved higher recall and lower error faster using relevance-driven clustering (BPS) in comparison to $K$-means and the Baseline.}
\label{fig:UserSurveyRecall}
\end{figure}





%However, 
%\begin{figure*}[t]
%\begin{centering}
%\includegraphics[width=7.5cm]{imgs/legend4}\\
%\subfigure[Natural disasters.] {\includegraphics[width=5.7cm]{imgs/nd_recall_steps}}
%\subfigure[Locations of the natural disasters.] {\includegraphics[width=5.7cm]{imgs/location_recall_steps}}
%\subfigure[Errors in the dates of the natural disasters.] {\includegraphics[width=5.7cm]{imgs/nd_time_deviation_recall_steps}}
%\par\end{centering}
%\caption{Performance of the algorithms measured at different elapsed times for each trial.}
%\label{fig:UserSurveyRecallStage}
%\end{figure*}



\subsection{Performance analysis}

The performance of the users for each algorithm was measured using cumulative recall for the type and location of the natural disasters in each trial.  Since there were three distinct natural disasters per trial,  perfect recall would require getting all three natural disaster types or locations correct.  Mean performance across all users with 95\% confidence intervals for cumulative recall of disaster type and location are respectively shown in Figures~\ref{fig:UserSurveyRecall}(a) and~\ref{fig:UserSurveyRecall}(b).  Users also had to enter their estimated starting date of each natural disaster, which we measure by absolute error assuming the maximum error for natural disasters that have not been submitted yet.  The mean performance of time estimation error across all users with 95\% confidence intervals is reported in Figure~\ref{fig:UserSurveyRecall}(c).  %Moreover, in Figure~\ref{fig:UserSurveyRecallStage} we report the progression of these metrics for each algorithm using boxplots at different elapsed times into the experiment -- at 300, 600, 900 and 1200 seconds.

While the %results of Figure~\ref{fig:UserSurveyRecall} 
number of users in our study and the 95\% confidence intervals 
do not permit strong claims of statistical significance regarding the quantitative portion of hypotheses H1 and H2, there are nonetheless consistent trends that emerge after 150 seconds into the trial:
%(i) 
We observe that on average, users achieved higher task recall and lower error faster when using the {\bf BPS} algorithm for clustering in comparison to {\bf $\mathbf{K}$-means} and the {\bf Baseline}.
% The performance obtained for this user study confirms clearly the results obtained in the off-line evaluation since the BPS algorithm outperforms $k$-means and the baseline in identifying the natural disasters, their locations, and their dates.
%(ii) In general, the clustering approach is clearly useful for this kind of task as both BPS and $k$-means outperformed the baseline that show all matching elements. 
%(iii) Clearly, the BPS algorithm required much fewer efforts and time to complete the task than for $k$-means and the baseline and that to reach a bit higher level of recall.
%(iv) 
%Further considering the performance progression shown in Figure~\ref{fig:UserSurveyRecallStage}
Furthermore, we clearly notice that with the {\bf BPS} algorithm, participants were able to achieve a higher performance at an earlier stage of the experiment than using the two other search algorithms. 

% As a conclusion for this evaluation, we notice that there is a similar outcome between the two evaluation methods -- the offline evaluation and the user study. Both evaluations confirm that the BPS algorithm
% the significant contribution of the BPS algorithm for this specific relevance-driven clustering task. Even though the off-line evaluation showed that $k$-means is inappropriate for this task, the performance obtained in this user study showed that it is effective to some extent. 




\begin{figure}[t!]
\begin{centering}
\includegraphics[width=7.5cm]{imgs/legend4}\\
\subfigure[Subscales rating. %``0'' means the user thinks that the workload for the considered dimension was ``Low'' and ``20'' means it was ``High''.
] {\includegraphics[width=4.6cm]{imgs/NASA_TLX}}
\subfigure[Overall rating.] {\includegraphics[width=3.8cm]{imgs/NASA_TLX_AVERAGE}}
\par\end{centering}
\caption{NASA Task Load Index (MD: Mental Demand, PD: Physical Demand, TD: Temporal Demand, OP: Own Performance, FR: Frustration, EF: Effort). }
\label{fig:NASA}
\end{figure}

\subsection{Surveys analysis}

With the previous quantitative measures of user performance indicating the advantage of relevance-driven clustering, we next proceed to evaluate the users' own opinions of each interface/algorithm as collected in the user surveys discussed in the methodology.

During the user study, each participant answered 7 different questionnaires -- a NASA Task Load Index (NASA-TLX)~\cite{HART1988139} and a System Usability Scale (SUS)~\cite{brooke1996sus} questionnaire after using each algorithm, plus a final questionnaire. We report key results below.




\subsubsection{NASA-TLX analysis}

The NASA-TLX questionnaire rates perceived workload in order to assess a task, system effectiveness or other aspects of performance.
The questionnaire includes questions on mental demand, physical demand, temporal demand, performance, frustration, and effort. NASA-TLX items are rated on a 20-point scale (1 = low workload, 20 = high workload, except for OP where 1 = perfect and 20 = failure). 
We show the NASA-TLX results obtained  for each subscale and for the overall ratings  in Figure~\ref{fig:NASA}.
Briefly, the mean overall NASA-TLX rating was $45.91\pm 8.86$ for {\bf BPS}, $54.75\pm 7.41$ for {\bf $\mathbf{K}$-means}, and $70.41\pm 8.27$ for the {\bf Baseline}. 
A Friedman's test revealed an overall significant difference ($\chi^2(3)= 16.113, p=0.003 < 0.05$).
Holm-Bonferroni corrected post-hoc analyses with Wilcoxon signed-rank tests 
revealed that the difference between all pairs was significant ($p < 0.05$). %The difference between BPS and $k$-means wasn't significant. Moreover, there were no significant differences in the sub-scales between BPS and $k$-means.


According to these results, we note that participants overall perceived the {\bf BPS} algorithm to be more effective at helping them complete their search task in comparison to using {\bf $\mathbf{K}$-means} or the {\bf Baseline}.  Global median rates of frustration and mental effort were around 10, which indicates that the task was neither too difficult, nor too easy.  Also, as the median rates of these two factors were lower for the two clustering methods than the baseline, we conclude that participants overall felt that clustering-based search provided a less frustrating and less mentally demanding interface for this task in comparison to the baseline, which displayed all search results. 
Finally, based on the task load rates where the {\bf BPS} algorithm performed the best, we believe that relevance-driven {\bf BPS} clustering provided the most effective approach for carrying out this kind of spatio-temporal search task.

% {\bf BPS} algorithm for clustering in comparison to {\bf $\mathbf{K}$-means} and the {\bf Baseline}.

\subsubsection{SUS analysis}
The SUS questionnaire gives a global view of subjective assessments of usability. The questionnaire contains questions related to the global effectiveness, efficiency, and satisfaction. The SUS items are rated on a 5-point scale (0 = strong disagreement and 5 = strong agreement). In Figure~\ref{fig:SUS}, we show the results obtained over all SUS scores  with 95\% confidence interval. The mean SUS score was $72.39\pm 9.77$ for {\bf BPS}, $70.83\pm 7.61$ for {\bf $\mathbf{K}$-means}, and $51.77\pm 9.07$ for the {\bf Baseline}. A Friedman's test revealed an overall significant difference ($\chi^2(3)= 9.053, p=0.010 < 0.05$).
Holm-Bonferroni corrected post hoc analyses with Wilcoxon signed-rank tests 
revealed that the difference between the two clustering methods and the baseline was significant ($p < 0.05$). The difference between {\bf BPS} and {\bf $\mathbf{K}$-means} wasn't significant and hence we can only infer from the SUS survey that the users preferred the clustering interface (i.e., {\bf BPS} and {\bf $\mathbf{K}$-means}) over the {\bf Baseline}.% display of all search results.


\begin{figure}[t!]
\begin{centering}
\includegraphics[width=7.5cm]{imgs/legend4}
{\includegraphics[width=7cm]{imgs/SUS_AVERAGE}}
\par\end{centering}
\caption{System Usability Scale overall rating.}
\label{fig:SUS}
\end{figure}



\begin{figure}[t!]
\begin{centering}
\includegraphics[width=7.5cm]{imgs/legend4}\\
{\includegraphics[width=8cm]{imgs/ranking}}
\par\end{centering}
\caption{Ranking of the algorithms by participants.}
\label{fig:UserSurveyRanking}
\end{figure}
\subsubsection{Final questionnaire analysis}
The final questionnaire that the users had to answer included questions on the advantages and disadvantages of each algorithm, plus a global ranking of the three algorithms. %Due to space limitations, we briefly summarize the top disadvantages of the BPS algorithm and advantages of the baseline method in Table~\ref{tbl:SurveyComments}.
%
% Preview source code for paragraph 0
%
%\begin{table}
%\caption{User survey comments.}
%\label{tbl:SurveyComments}
%\begin{centering}
%\begin{tabular}{|>{\centering}p{4cm}|>{\centering}p{4cm}|}
%\hline 
%\textbf{Disadvantages of BPS} & \textbf{Advantages of Baseline}\tabularnewline
%\hline 
%\hline 
%Clusters that have huge time-spend are most likely useless. & Simple, easy. Using %just the time sliders it was very easy to detect
%the disasters. \tabularnewline
%\hline 
%Does not show up all the data points.  & Using the timeslider, I found it more %useful than the filters. \tabularnewline
%\cline{1-1} 
%The interface takes more time to learn.  & \tabularnewline
%\hline 
%\end{tabular}
%\par\end{centering}
%\end{table}
%
The ranking of the algorithms provided by users is shown in Figure \ref{fig:UserSurveyRanking}. 
We note that 18 users out of 24 ranked {\bf BPS} as being the best algorithm, and 19 users out of 24 ranked the {\bf Baseline} approach as being the least helpful for the task.  Hence there is strong evidence for user preference of {\bf BPS} over {\bf $\mathbf{K}$-means} and for both {\bf BPS} and {\bf $\mathbf{K}$-means} over the {\bf Baseline}, partially confirming the preference hypotheses of H1 and H2.  
%However, we note that two users mentioned that they still prefer the baseline approach. 
%This is probably due to the fact that they are used to work with a conventional search approach, and that they couldn't digest this new search clustering paradigm. 
Finally, in the free response survey section, several users specifically reported the %utility,
ease and precision provided by the trial with the {\bf BPS} algorithm  assigned.% to complete this search task.

















