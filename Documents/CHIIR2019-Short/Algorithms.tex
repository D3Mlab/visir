In this section, we  first briefly describe a scalable greedy-based algorithm to efficiently optimize EF1 objective, and then, we describe a MILP-based approach (Mixed Integer Linear Programming) to benchmark the performance of the proposed greedy algorithm on moderate-sized problems.

%have defined the EF1 objective for relevance-driven clustering optimization, we need to find efficient ways to (approximately) optimize it.  To this end, this section first describes two scalable and efficient greedy algorithms to optimize EF1.
%used to build filters setting for querying large data graphs in order to retrieve relevant UI elements that are of interest to the users.  
%Following this, we describe an optimization-based approach based on Mixed Integer Linear Programming (MILP) to benchmark the performance of the proposed greedy algorithms on moderate-sized problems.

\subsection{Greedy relevance-driven clustering}

% Should discuss greedy algorithm generically as having a metric and at each step
% a choice of k restrictions, which are each scored against the metric with the
% highest score chosen at each step.  Then each individual filter only has to
% specify what the choices are and how the choice restricts the set of emails
% selected.  What is a good succinct notation for this?

As discussed previously, we assume that three types of clustering ``parameters'' are used to optimize clusters of relevant information: Keyword, Time, and Space.  A cluster is generated by \emph{conjoining} these three selection parameters. In the following, we describe how to greedily optimize each of these selection parameters, and then how to combine them for producing relevance-driven clusters.

\subsubsection{Greedy Keyword Selection algorithm}

Given a set of information elements matching a user query, the Greedy Keyword Selection algorithm aims to select a set of keywords  in order to exclude a subset of elements containing these keywords for the purpose of maximizing the EF1-Score. 
Formally, the algorithm aims to select an
optimal subset of $k$ terms $T_{k}^{*}\subset E$ (where $|T_{k}^{*}|=k$ and $E$ is the initial set of elements) to exclude elements containing these keywords for optimizing the EF1-score. This is achieved by
building $T_{k}^{*}$ in a greedy manner by choosing the next optimal
term $t_{k}^{*}$ given the previous set of optimal term selections
$T_{k-1}^{*}=\{t_{1}^{*},\ldots,t_{k-1}^{*}\}$ (assuming $T_{0}^{*}=\emptyset$)
using the following selection criterion:
\begin{equation}
t_{k}^{*}=\argmax_{t_{k}\notin T_{k-1}^{*}}\hspace{-0.3mm}[EF1(E^{*} \textrm{ that don't contain } \{t_{1}^{*},\dots t_{k}^{*}\})]
\end{equation}
where  $E^{*}$ is a subset of the initial element set $E$ that don't contain the keywords $\{t_{1}^{*},\dots t_{k}^{*}\}$. 
%In order to reduce the keyword search space, we propose to use the top 100 terms ranked using Mutual Information to identify the keywords that are predictive of the ``supervised'' relevance measure.  We remark that other metrics like frequency would be more appropriate for unsupervised tasks.  We further remark that we have chosen to use a negation query as a means to effectively prune the content as more terms are selected.  
%mainly for technical reasons as a negation query generates queries with much fewer terms. 
%The best indexing strategy to support this greedy search is the inverted index data structure~\cite{Zobel2006}.

\subsubsection{Greedy Time Selection algorithm}

The idea behind the time-based greedy selection algorithm is as simple as finding a time window range $[t_{start},t_{end}]$, which allows to select a subset of elements $E^{*}\subseteq E$ falling in that time window, with $E^{*}$ having the highest EF1-Score. 
Formally, given a list of elements $E=\{j_{t_1}\leq \dots \leq j_{t_n}\}$, where "$\leq$" specifies the timestamp order, we propose to use binary partitioning search (BPS) to find the best set that optimizes EF1.  %Hence, instead of removing a single element $j$ at each iteration, this algorithm operates by selecting between two distinct alternatives (binary partitions) at each iteration. 
%\subfour{(a) Naive greedy algorithm:} First, at each iteration of this algorithm, an early ranked element $j_{t_i}$ is removed, and then, the remaining set is assessed using EF1-Score. If the remaining set has a lower EF1-score value than the set of the previous iteration, the algorithm assigns $t_i$ to the lower time bound of the time query, i.e., $t_{start}=t_i$. 
%Next, the algorithm does the same set of operations, by removing at each iteration a lastly ranked element  $j_{t_i}$, and by stopping once the removal of  $j_{t_i}$ causes a decrease in the EF1-Score value. Then, the algorithm assigns $t_i$ to the upper time bound of the time query, i.e., $t_{end}=t_i$. 
%Lastly, the algorithm returns the  time query $Q_t=[t_{start},t_{end}]$, with obviously $EF1(E^{*} \textrm{ that satisfies } Q_t=[t_{start},t_{end}]) \geq EF1(E)$.

%\subfour{(b) Binary Partition Search algorithm:} Large datasets with sparse positive data (e.g., 0.5\% of alerts in a security graph) will cause the previous algorithm to take a large number of iterations to terminate since it greedily adjusts selection settings in a minimal way at each step.  A way to address this problem is to use binary partitioning search (BPS).  Hence, instead of removing a single element $j$ at each iteration, this algorithm operates by selecting between two distinct alternatives (binary partitions) at each iteration. 

%\begin{algorithm}[t]
%\scriptsize
%\caption{Binary Partition Search (BPS) Algorithm}
%\SetAlgoLined
%\SetKwData{Left}{left}\SetKwData{This}{this}\SetKwData{Up}{up}
%\SetKwFunction{Union}{Union}\SetKwFunction{FindCompress}{FindCompress}
%\SetKwInOut{Input}{input}\SetKwInOut{Output}{output}
%\Input{A set of ordered elements $E=\{j_{v_1} \dots j_{v_n}\}$ }
%\Output{A timestamp $t$;}
%\BlankLine
%\label{alg:Dichotomy}

%$v_{min}=v_1$; $v_{max}=v_n$;  $v_{mid}=\tfrac{v_1+v_n}{2}$;

%\While {$v_{min}!=v_{mid}!=v_{max}$}{

%\eIf{$[EF1(\{j_{v_{min}} \dots  j_{v_n}\}) \geq EF1(\{j_{v_{mid}} \dots  j_{v_n}\})]$}{
%$v_{max}=v_{mid}$;
%$v_{mid}=\tfrac{v_{min}+v_{mid}}{2}$;
%}{
%$v_{min}=v_{mid}$; 
%$v_{mid}=\tfrac{v_{min}+v_{max}}{2}$;
%}
%}
%\Return  $v_{mid}$;

%\label{alg:return}
%\end{algorithm} 


%Therefore, the algorithm first sets the values $t_{min}=t_1$, $t_{max}=t_n$,  and $t_{mid}=\tfrac{t_1+t_n}{2}$. Then, for each iteration, if $[EF1(\{d_{t_{min}}\leq \dots \leq d_{t_n}\}) \geq EF1(\{d_{t_{mid}}\leq \dots \leq d_{t_n}\})]$, the algorithm sets $t_{max}=t_{mid}$, $t_{mid}=\tfrac{t_{min}+t_{mid}}{2}$ and makes a new iteration, else, the algorithm sets $t_{min}=t_{mid}$,  $t_{mid}=\tfrac{t_{min}+t_{max}}{2}$  and makes a new iteration. The algorithm keeps iterating until $t_{min}=t_{mid}=t_{max}$, where it assigns $t_{mid}$ to the lower time bound of the time query, i.e., $t_{start}=t_{mid}$.


%As an example of the BPS approach for the time selection, the algorithm first sorts $E$ in increasing order of time stamp. Then, it applies the procedure described by Algorithm \ref{alg:Dichotomy}. This procedure will return  the lower time bound of the time window, i.e., $t_{start}=t_{mid}$.
%Next, the algorithm sorts $E$ in decreasing order of time stamp, and then, it applies again the procedure described by Algorithm \ref{alg:Dichotomy} to get the upper time bound of the time window, i.e., $t_{end}=t_i$. Lastly, the algorithm returns the  time window $Q_t=[t_{start},t_{end}]$, such that $EF1(E^{*} \textrm{ that satisfies } Q_t=[t_{start},t_{end}]) \geq EF1(E)$. Note that this algorithm proceeds in a total of $log(n)$ iterations in the best case, and $2\times log(n)$ iterations in the worst case.

%For both the naive and time-based greedy algorithms, we use the red-black tree as the indexing data structure \cite{Guibas1978}.


\subsubsection{Greedy Spatial Selection algorithm}

The aim of this algorithm is to return coordinates $[(x_{min},y_{min}),(x_{max},y_{max})]$ representing the EF1-Score maximizing bounding box represented by the lower and upper bound coordinates -- respectively $(x_{min},y_{min})$ and $(x_{max},y_{max})$. This 2D problem is similar to the previous one dimensional problem of finding the best time window. Therefore, we also propose to use a BPS  on the x-axis to determine $(x_{min},x_{max})$, then on the y-axis to determine $(y_{min},y_{max})$. 
% We need to ditch some citations and this removes the complaint that we did not provide all details -- we claim it is obvious.  -Scott
%We omit the description of these two algorithms for lack of space, but a detailed description can be found in the technical report~\cite{Bouadjenek2018}.
%We use the R-tree as a data structure for indexing multi-dimensional continuous data~\cite{Guttman1984}.


\subsubsection{Relevance-driven clustering algorithm} To obtain a cluster combining the above selection parameters, we propose a greedy algorithm, which at each iteration applies all the above selection algorithms and chooses the one that improves the most EF1.  The selected cluster is updated with its new setting and the iteration continues.  Iterations termination when no selection algorithm can unilaterally improve EF1 and the final cluster is returned.
%to, theselects the best sub-filter to apply (according to its reduction in EF1-Score), then checks if that filter improves the EF1-Score. If so, the algorithm continues with the updated filter settings, otherwise, it terminates. 
% Don't get the following -- seems non-essential, at least for submission.  -Scott
%Note that here, we use  $k=1$ for the keywords greedy algorithm.
%The algorithm will then determine a sequence of filters to apply on the initial set, and the final query is then built by combining these filters by types.

 %such as: $\{Keyword \to Time -> Position \to Time \to Keyword\} $

% I don't follow this example, but I think the algorithm is intuitive enough and we need space.  -Scott
% For example, let's suppose the algorithm determines the following filter sequence: $\{Q_{k_1}=\{ \neg natural\} \to Q_{t_1}=[50,2030] \to Q_{p_1}=[(10,60), (50,100)] \to Q_{t_2}=[60,1230] \to Q_{k_2}=\{ \neg fictive\}  \to  Q_{p_2}=[(10,60), (30,85)] \to  Q_{t_3}=[60,800] \}$. 
% The final query is built by combining these filters by types as follows: $Q_k=Q_{k_1} \cup Q_{k_2}=\{\neg natural,\neg fictive\}$, $Q_t =Q_{t_1} \cap Q_{t_2} \cap Q_{t_3}=[60,800]$, and  $ Q_p=Q_{p_1}  \cap Q_{p_2}=[(10,60),$ $(30,85)]$, which gives $Q=[Q_k=\{\neg natural,\neg fictive\}\wedge Q_t=[60,800]\wedge Q_p = [(10,60),$ $(30,85)]]$.

%Finally, we note that our relevance-driven clustering algorithm can use the Greedy Keyword selection algorithm with the naive Greedy Time and Spatial Selection algorithms, which we refer to experimentally as {\bf Greedy}, or the Binary Partition Search variants, which we refer to experimentally as {\bf BPS}.  
%time and position naive greedy algorithms to which we refer as Greedy Algorithm, or the keywords greedy algorithm with the time and position Binary Partition Search algorithms to which we refer as BPS Algorithm.



\subsection{Optimal MILP for Benchmarking}

%Next, we propose an exact Mixed Integer Linear Progamming (MILP) optimization-based formulation to maximize EF1 and provide a benchmark for evaluating our two relevance-driven clustering algorithms (Greedy and BPS).  %Given the trivial solution of optimizing the expected precision (singleton) and expected recall (the whole collection), we consider in the following only the optimization of EF1.  

%\subsubsection{Fractional MILP Formulation} \hfill \\
We first reformulate EF1 objective by replacing the global sum of scores of all information elements with a constant $C = \sum_{j=1}^m S(j)$:
\begin{equation}
\begin{aligned}
    \emph{$EF1$} &= \dfrac{2\times \sum_{j=1}^m S(j)I(j)}{\sum_{j=1}^m I(j) + \sum_{j=1}^m S(j)} = \dfrac{2 \times \sum_{j=1}^m S(j)I(j)}{\sum_{j=1}^m I(j) + C}
\end{aligned}
\end{equation}

In order to obtain the EF1-optimal cluster, we let binary variables $\emph{I\textsubscript{f}}($j$) \in \{0, 1\}$ to indicate whether an element $j$ is selected by each parameter of the cluster (i.e., $I(j)=1$), $j$ must be selected in all selection constrains.  This leads to the following fractional MILP:
%intend to directly optimize the EF1 metric in terms of decision indication variables  for filter setting as follows:
% Don't use I(i)... confusing!  
%\begin{align}
%\max_{\textit{filter vars}} \;\;
%& \dfrac{\sum_{j=1}^m S(i)I(j)}{\sum_{j=1}^m I(j) + C} \nonumber \\
%& \textrm{subject to constraints between {\it filter vars} and $I(j)$}   
%\end{align}
\begin{equation}
\begin{aligned}
& \underset{I_{\mathit{filter}}(j)}{\text{maximize}}
& & \dfrac{\sum_{j=1}^m S(j)I(j)}{\sum_{j=1}^m I(j) + C} \\
& s.t
& & I(j) = \bigwedge I_{\mathit{f}}(j) \\
\end{aligned} \label{eq:frac_milp}
\end{equation}

% Not necessary.  -Scott
%Note that our goal is to optimize the element set in terms of \emph{filters settings}. This means elements in the interface sharing the same property needs to be simultaneously added to the selected set. This is a unique property of our filter-based UI problem, which is different from the independent retrieval of each document in standard IR system.

%\subsubsection{Transformation to a MILP} \hfill \\
While there are no direct solvers for fractional MILPs, we can transform Equation~\eqref{eq:frac_milp} into a pure MILP form for which we have efficient and optimal solvers.  To do this, we use the Charnes-Cooper method \cite{Charnes1962} and Glover linearization method \cite{Glover1975} with big-M constraints, where auxiliary variables \emph{w(j)} and \emph{u} are  introduced\footnote{https://optimization.mccormick.northwestern.edu/index.php/Mixed-integer\_linear\_fractional\_programming\_(MILFP)}. Here, $w(j)$ is defined as $w(j)=I(j)\times u$ with $u = 1 / (\sum_{j=1}^m I(j) + C)$.
%\begin{equation}
%u = \dfrac{1}{\sum_{j=1}^m I(j) + C}
%\end{equation}

Then, the EF1 optimization problem is able to be transformed into the following MILP problem:
\begin{equation}
\begin{aligned}
& \underset{w,u}{\text{maximize}}
& & \sum_{j=1}^m S(j)w(j) \\
& s.t
& & \sum_{j=1}^m w(j) + uC = 1 \\
& & & w(j) \leqslant u, \quad w(j) \leqslant M\times I(j)  \\
& & & w(j) \geqslant u - M\times [1-I(j)] \\
& & & u > 0,  \quad I(j) \in \{0, 1\}, \quad w(j) \geqslant 0 \label{eq:milp}
\end{aligned}
\end{equation}

Finally, several constraints are added to the objective~\eqref{eq:milp} to represent time, position, and keywords inclusive constraints.
%\subsubsection{Constraints} \hfill \\
%As our goal is to select elements through three clustering parameters, we add three constraints to the above optimization.
%\begin{enumerate}
%\item {\bf Time Parameter Constraint:} a two-element tuple ($t_{start}$, $t_{end}$) indicating respectively the start and the end of the time window.
%\begin{equation}
%\begin{aligned}
%  I_{\mathit{time}}(j) &=
%   \begin{cases}
%     1, & \text{if $(t_{start} \leqslant t(j)) \land (t(j) \leqslant t_{end})$}  \\
%     0, & \text{otherwise}
%  \end{cases} \label{eq:cons1}
%\end{aligned}
%\end{equation}

%\item {\bf Spatial Parameter Constraint:} a four-element tuple ($x_{min}$, $y_{min}$, $x_{max}$, $y_{max}$) to create a bounding box selection in visualization interface.
%\begin{equation}
%\begin{aligned}
%I_{\mathit{pos}}(j) & =\begin{cases}
%1, & \text{if \ensuremath{(x_{min}\leqslant x(j))\land(x(j)\leqslant x_{max})\land}}\\
% & (y_{min}\leqslant y(j))\land(y(j)\leqslant y_{max})\\
%0, & \text{otherwise}
%\end{cases} \label{eq:cons2}
%\end{aligned}
%\end{equation}

%\item {\bf Keyword Parameter Constraint:} a boolean vector of terms $t^*_k$ with size $m$ - the size of the dictionary of $GC$.
%\begin{equation}
%  I_{\mathit{term}}(j) = \bigwedge_{t^*_k \in j} t^*_k \qquad \textnormal{for s = 1, 2, $\cdots$, m} \label{eq:cons3}
%\end{equation}
%All terms with $I_{\mathit{term}}=0$ are included in the negation query.


%\item {\bf Clustering Selection Constraints:} for information element $j$ to be selected globally, it must be simultaneously selected by the three selection constrains.
%, all filter constraints have to be satisfied in this element. In others words, an AND operator is required between all the sub-filter constraints.
%{\bf TODO: mention how to apply to all filters... need to say an email j is selected if \emph{all} filters say it is selected, so an AND constraint.  \textcolor{red}{[PLEASE COMPLETE YIHAO]}.
%} 
%\begin{equation}
%  I(j) = I_{\mathit{time}}(j) \land I_{\mathit{pos}}(j) \land I_{\mathit{keyword}}(j) \label{eq:cons4}
%\end{equation}

%\end{enumerate}
%We refer to the above MILP formulation in~\eqref{eq:milp} with all selection parameter constraints~\eqref{eq:cons1}--\eqref{eq:cons4} as the {\bf Optimal} cluster.

\subsection{Multiple Cluster Selection Wrapper}

%The Global Filter selects a single best filter setting.  However, what if we want to display multiple possible Global Filters.  
In practice a single cluster chosen by the previously described algorithms will narrow the user in a single \textquotedblleft information perspective\textquotedblright{}. However, there will likely be multiple perspectives and so the user should have a choice of multiple clusters.  
Consider Figure~\ref{Fig:UseCase}: this actually shows three different spatial bounding boxes corresponding to three different events provided by three clusters.
Here, we provide a greedy approach for providing a ranked list of clusters that works with any of the previously defined algorithms -- Greedy or Optimal.%); we leave it to future work to develop improved filtering and ranking methods for multiple filters.
The algorithm itself is quite simple.  After the first cluster is produced, all selected elements in that cluster have their scores $S(j)$ zeroed out.  The relevance-driven clustering algorithm is then run again, where it will inherently focus on a different content set.  
%This procedure is repeated until the desired number of filters is reached, or a metric / coverage score for high scoring content is reached.  
%The user should then be able to choose among the multiple filters in the VID. 	










